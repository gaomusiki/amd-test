{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ref.modeling import (\n",
    "    AttnQKVPackFormat,\n",
    "    AttnQKVLayout,\n",
    "    OfflineSlidingWindowAttn, \n",
    "    OnlineSlidingWindowAttn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((100,))\n",
    "b = (1 + a.exp()).log()\n",
    "c = a.exp().log1p()\n",
    "d = F.softplus(a)\n",
    "\n",
    "torch.allclose(b, c, equal_nan=True); torch.allclose(b, d, equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((100, 10000), dtype=torch.float32)\n",
    "b = torch.rsqrt(a)\n",
    "c = torch.rsqrt(a.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(b.cuda(), c, equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_offline_attn_args(\n",
    "    b: int,\n",
    "    sq: int,\n",
    "    skv: int,\n",
    "    hq: int,\n",
    "    hkv: int,\n",
    "    hd: int,\n",
    "    qkv_pack_format: AttnQKVPackFormat,\n",
    "    qkv_layout: AttnQKVLayout,\n",
    "    seqlens_q = None,\n",
    "    seqlens_kv = None,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    device: str = \"cpu\",\n",
    "    seed: int = 42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    q = torch.randn((b, sq, hq, hd), dtype=dtype, device=device)\n",
    "    k = torch.randn((b, skv, hkv, hd), dtype=dtype, device=device)\n",
    "    v = torch.randn((b, skv, hkv, hd), dtype=dtype, device=device)\n",
    "    \n",
    "    if qkv_layout == AttnQKVLayout.THD:\n",
    "        assert seqlens_q is not None, \"THD layout requires cu_seqlens_q\"\n",
    "        assert seqlens_kv is not None, \"THD layout requires cu_seqlens_kv\"\n",
    "        \n",
    "        cu_seqlens_q, cu_seqlens_kv =[\n",
    "            torch.concat([\n",
    "                torch.zeros(1, dtype=torch.int32, device=device),\n",
    "                torch.tensor(x, dtype=torch.int32, device=device).cumsum(dim=0)\n",
    "            ], dim=0)\n",
    "            for x in (seqlens_q, seqlens_kv)\n",
    "        ]\n",
    "        \n",
    "        q, k, v = [\n",
    "            x.view(-1, *x.shape[-2:]).contiguous() \n",
    "            for x in (q, k, v)\n",
    "        ]\n",
    "    else:\n",
    "        assert seqlens_q is None, \"QKV layout does not require cu_seqlens_q\"\n",
    "        assert seqlens_kv is None, \"QKV layout does not require cu_seqlens_kv\"\n",
    "        cu_seqlens_q, cu_seqlens_kv = None, None\n",
    "        \n",
    "        if qkv_layout == AttnQKVLayout.SBHD:\n",
    "            q, k, v = [\n",
    "                x.transpose(0, 1).contiguous() \n",
    "                for x in (q, k, v)\n",
    "            ]\n",
    "    \n",
    "    if qkv_pack_format == AttnQKVPackFormat.QKV:\n",
    "        assert sq == skv, \"QKV pack format requires sq == skv\"\n",
    "        q = torch.concat((q, k, v), dim=-2)\n",
    "        k, v = None, None\n",
    "    elif qkv_pack_format == AttnQKVPackFormat.Q_KV:\n",
    "        k = torch.concat((k, v), dim=-2)\n",
    "        v = None\n",
    "    \n",
    "    return q, k, v, cu_seqlens_q, cu_seqlens_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_online_attn_args(\n",
    "    b: int,\n",
    "    sq: int,\n",
    "    skv: int,\n",
    "    hq: int,\n",
    "    hkv: int,\n",
    "    hd: int,\n",
    "    bq: int,\n",
    "    bkv: int,\n",
    "    bqi: int,\n",
    "    bkvi: int,\n",
    "    dtype: torch.dtype = torch.float32,\n",
    "    device: str = \"cpu\",\n",
    "    seed: int = 42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    q = torch.randn((b, sq, hq, hd), dtype=dtype, device=device)\n",
    "    k = torch.randn((b, skv, hkv, hd), dtype=dtype, device=device)\n",
    "    v = torch.randn((b, skv, hkv, hd), dtype=dtype, device=device)\n",
    "    global_o = torch.randn_like(q)\n",
    "    global_lse = torch.rand((b, hq, sq), dtype=torch.float32, device=device)\n",
    "    \n",
    "    nbq = (sq + bq - 1) // bq\n",
    "    nbk = (skv + bkv - 1) // bkv\n",
    "\n",
    "    q = F.pad(q, pad=(0, 0, 0, 0, 0, nbq*bq - sq), mode=\"constant\", value=0)\n",
    "    k = F.pad(k, pad=(0, 0, 0, 0, 0, nbk*bkv - skv), mode=\"constant\", value=0)\n",
    "    v = F.pad(v, pad=(0, 0, 0, 0, 0, nbk*bkv - skv), mode=\"constant\", value=0)\n",
    "    \n",
    "    q = q[:, bqi*bq:(bqi+1)*bq, :, :]\n",
    "    k = k[:, bkvi*bkv:(bkvi+1)*bkv, :, :]\n",
    "    v = v[:, bkvi*bkv:(bkvi+1)*bkv, :, :]\n",
    "    \n",
    "    return q, k, v, global_o, global_lse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## task1 - toy case1\n",
    "\n",
    "# b = 1\n",
    "# sq, skv = 6, 6\n",
    "# hq, hkv = 1, 1\n",
    "# hd = 4\n",
    "\n",
    "# window_size = None\n",
    "# causal = True\n",
    "\n",
    "# softmax_dropout_rate = 0.1\n",
    "# softmax_scale = None\n",
    "# softmax_cap = None\n",
    "# softmax_temp = 0.8\n",
    "# softmax_clip_range = (-0.03, 1.03)\n",
    "\n",
    "# qkv_pack_format = AttnQKVPackFormat.QKV\n",
    "# qkv_layout = AttnQKVLayout.SBHD\n",
    "\n",
    "# seqlens_q = None\n",
    "# seqlens_kv = None\n",
    "\n",
    "# group_size = 1\n",
    "# init_range = (-1.1, 1.1)\n",
    "\n",
    "# act_dtype=torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### task1 - toy case2\n",
    "\n",
    "# b = 1\n",
    "# sq, skv = 7, 5\n",
    "# hq, hkv = 2, 1\n",
    "# hd = 4\n",
    "\n",
    "# window_size = 1\n",
    "# causal = False\n",
    "\n",
    "# softmax_dropout_rate = 0.0\n",
    "# softmax_scale = None\n",
    "# softmax_cap = 10\n",
    "# softmax_temp = 1.0\n",
    "# softmax_clip_range = (-0.01, 1.01)\n",
    "\n",
    "# qkv_pack_format = AttnQKVPackFormat.Q_KV\n",
    "# qkv_layout = AttnQKVLayout.THD\n",
    "\n",
    "# seqlens_q = [1, 2, 4]\n",
    "# seqlens_kv = [2, 2, 1]\n",
    "\n",
    "# group_size = 2\n",
    "# init_range = (-1.2, 1.2)\n",
    "\n",
    "# act_dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### task2 - toy case1\n",
    "\n",
    "# b = 1\n",
    "# sq, skv = 7, 5\n",
    "# hq, hkv = 1, 1\n",
    "# hd = 4\n",
    "\n",
    "# bq, bkv = 3, 2\n",
    "# bqi_, bkvi_ = 1, 1\n",
    "\n",
    "# window_size = 2\n",
    "# causal = True\n",
    "\n",
    "# softmax_scale = None\n",
    "# softmax_dropout_rate = 0.0\n",
    "# softmax_cap = 10\n",
    "# softmax_temp = 1.0\n",
    "# softmax_clip_range = (0., 1.)\n",
    "\n",
    "# qkv_pack_format = AttnQKVPackFormat.Q_K_V\n",
    "# qkv_layout = AttnQKVLayout.BSHD\n",
    "\n",
    "# seqlens_q = None\n",
    "# seqlens_kv = None\n",
    "\n",
    "# group_size = 2\n",
    "# init_range = (-1.05, 1.05)\n",
    "\n",
    "# act_dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### task2 - toy case2\n",
    "\n",
    "b = 1\n",
    "sq, skv = 7, 5\n",
    "hq, hkv = 1, 1\n",
    "hd = 4\n",
    "\n",
    "bq, bkv = 3, 2\n",
    "bqi_, bkvi_ = 2, 0\n",
    "\n",
    "window_size = 1\n",
    "causal = False\n",
    "\n",
    "softmax_scale = None\n",
    "softmax_dropout_rate = 0.0\n",
    "softmax_cap = None\n",
    "softmax_temp = 0.9\n",
    "softmax_clip_range = (0., 1.)\n",
    "\n",
    "qkv_pack_format = AttnQKVPackFormat.Q_K_V\n",
    "qkv_layout = AttnQKVLayout.BSHD\n",
    "\n",
    "seqlens_q = None\n",
    "seqlens_kv = None\n",
    "\n",
    "group_size = 1\n",
    "init_range = (-1.25, 1.25)\n",
    "\n",
    "act_dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, cu_seqlens_q, cu_seqlens_kv = \\\n",
    "    construct_offline_attn_args(\n",
    "        b, sq, skv, hq, hkv, hd, \n",
    "        qkv_pack_format, qkv_layout,\n",
    "        seqlens_q, seqlens_kv,\n",
    "        dtype=act_dtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_kwargs = {\n",
    "    \"head_dim\": hd,\n",
    "    \"num_q_head\": hq,\n",
    "    \"num_kv_head\": hkv,\n",
    "    \"window_size\": window_size,\n",
    "    \"causal\": causal,\n",
    "    \"softmax_scale\": softmax_scale,\n",
    "    \"softmax_cap\": softmax_cap,\n",
    "    \"softmax_temp\": softmax_temp,\n",
    "    \"group_size\": group_size,\n",
    "    \"eps\": 1e-5,\n",
    "    \"init_range\": init_range,\n",
    "    \"init_seed\": 42,\n",
    "    \"dtype\": torch.float32,\n",
    "    \"device\": \"cpu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_swa = OfflineSlidingWindowAttn(\n",
    "    qkv_pack_format=qkv_pack_format,\n",
    "    qkv_layout=qkv_layout,\n",
    "    softmax_dropout_rate=softmax_dropout_rate,\n",
    "    softmax_dropout_seed=42,\n",
    "    softmax_clip_range=softmax_clip_range,\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
       " \n",
       "          [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
       " \n",
       "          [[ 0.0766, -0.1375, -0.8620, -0.5507]],\n",
       " \n",
       "          [[-0.1484,  0.8362,  0.2361, -0.3500]],\n",
       " \n",
       "          [[-0.3544,  0.7886,  0.1344, -0.2238]],\n",
       " \n",
       "          [[-0.1641,  0.8789,  0.1191,  0.0121]]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " torch.Size([1, 7, 1, 4]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = off_swa(q, k, v, cu_seqlens_q, cu_seqlens_kv)\n",
    "o, o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_swa = OnlineSlidingWindowAttn(\n",
    "    block_size_q=bq,\n",
    "    block_size_kv=bkv,\n",
    "    seqlen_q=sq,\n",
    "    seqlen_kv=skv,\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-inf, -inf, -inf, -inf, -inf, -inf],\n",
       "           [0., -inf, -inf, -inf, -inf, -inf],\n",
       "           [0., 0., -inf, -inf, -inf, -inf],\n",
       "           [0., 0., 0., -inf, -inf, -inf],\n",
       "           [-inf, 0., 0., 0., -inf, -inf],\n",
       "           [-inf, -inf, 0., 0., 0., -inf],\n",
       "           [-inf, -inf, -inf, 0., 0., -inf],\n",
       "           [-inf, -inf, -inf, -inf, -inf, -inf],\n",
       "           [-inf, -inf, -inf, -inf, -inf, -inf]]]]),\n",
       " torch.Size([1, 1, 9, 6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_swa.global_attn_mask, on_swa.global_attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-inf, -inf, -inf, -inf, -inf],\n",
       "          [0., -inf, -inf, -inf, -inf],\n",
       "          [0., 0., -inf, -inf, -inf],\n",
       "          [0., 0., 0., -inf, -inf],\n",
       "          [-inf, 0., 0., 0., -inf],\n",
       "          [-inf, -inf, 0., 0., 0.],\n",
       "          [-inf, -inf, -inf, 0., 0.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_swa.global_attn_mask[..., :7, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.9269,  1.4873,  0.9007, -2.1055]],\n",
       " \n",
       "          [[ 0.6784, -1.2345, -0.0431, -1.6047]],\n",
       " \n",
       "          [[-0.7521,  1.6487, -0.3925, -1.4036]],\n",
       " \n",
       "          [[-1.1109,  0.0915, -2.3169, -0.2168]],\n",
       " \n",
       "          [[-1.3847, -0.8712, -0.2234,  1.7174]],\n",
       " \n",
       "          [[-0.5920, -0.0631, -0.8286,  0.3309]],\n",
       " \n",
       "          [[-1.5576,  0.9956, -0.8798, -0.6011]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000]]]]),\n",
       " tensor([[[[ 1.3123,  0.6872, -1.0892, -0.3553]],\n",
       " \n",
       "          [[ 1.4451,  0.8564,  2.2181,  0.5232]],\n",
       " \n",
       "          [[ 0.3466, -0.1973, -1.0546,  1.2780]],\n",
       " \n",
       "          [[-0.1722,  0.5238,  0.0566,  0.4263]],\n",
       " \n",
       "          [[ 0.5750, -0.6417, -2.2064, -0.7508]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000]]]]),\n",
       " tensor([[[[ 1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01]],\n",
       " \n",
       "          [[ 6.4076e-01,  5.8325e-01,  1.0669e+00, -4.5015e-01]],\n",
       " \n",
       "          [[-6.7875e-01,  5.7432e-01,  1.8775e-01, -3.5762e-01]],\n",
       " \n",
       "          [[ 2.6491e-01,  1.2732e+00, -1.3109e-03, -3.0360e-01]],\n",
       " \n",
       "          [[-9.8644e-01,  1.2330e-01,  3.4987e-01,  6.1728e-01]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]]),\n",
       " tensor([[[[0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0., 0.]]]]),\n",
       " tensor([[[-inf, -inf, -inf, -inf, -inf, -inf, -inf]]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbq = (sq + bq - 1) // bq\n",
    "nbk = (skv + bkv - 1) // bkv\n",
    "\n",
    "q = F.pad(q, pad=(0, 0, 0, 0, 0, nbq*bq - sq), mode=\"constant\", value=0)\n",
    "k = F.pad(k, pad=(0, 0, 0, 0, 0, nbk*bkv - skv), mode=\"constant\", value=0)\n",
    "v = F.pad(v, pad=(0, 0, 0, 0, 0, nbk*bkv - skv), mode=\"constant\", value=0)\n",
    "\n",
    "o_ = torch.zeros_like(o)\n",
    "lse_ = torch.zeros((b, hq, sq), dtype=o_.dtype, device=o_.device)\n",
    "lse_.fill_(float(\"-inf\"))\n",
    "\n",
    "q, k, v, o_, lse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block idx: q: 0 | kv: 0\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614,   -inf,   -inf,   -inf,   -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 0 | kv: 1\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614,   -inf,   -inf,   -inf,   -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 0 | kv: 2\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614,   -inf,   -inf,   -inf,   -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 1 | kv: 0\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9226, -0.5619]],\n",
      "\n",
      "         [[ 0.6408,  0.5832,  1.0669, -0.4502]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[   -inf,  0.6894,  1.0614,  1.0610, -0.4205,    -inf,    -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 1 | kv: 1\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0766, -0.1375, -0.8620, -0.5507]],\n",
      "\n",
      "         [[-0.1484,  0.8362,  0.2361, -0.3500]],\n",
      "\n",
      "         [[-0.2714,  0.8760,  0.1061, -0.3343]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614, 1.1172, 1.5804, 1.4350,   -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 1 | kv: 2\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0766, -0.1375, -0.8620, -0.5507]],\n",
      "\n",
      "         [[-0.1484,  0.8362,  0.2361, -0.3500]],\n",
      "\n",
      "         [[-0.3544,  0.7886,  0.1344, -0.2238]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614, 1.1172, 1.5804, 1.5584,   -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 2 | kv: 0\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0766, -0.1375, -0.8620, -0.5507]],\n",
      "\n",
      "         [[-0.1484,  0.8362,  0.2361, -0.3500]],\n",
      "\n",
      "         [[-0.3544,  0.7886,  0.1344, -0.2238]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614, 1.1172, 1.5804, 1.5584,   -inf]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 2 | kv: 1\n",
      "all global o: tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01]],\n",
      "\n",
      "         [[ 1.2022e-01, -1.7867e-01, -9.2269e-01, -5.6190e-01]],\n",
      "\n",
      "         [[ 7.6579e-02, -1.3751e-01, -8.6197e-01, -5.5073e-01]],\n",
      "\n",
      "         [[-1.4836e-01,  8.3620e-01,  2.3610e-01, -3.4998e-01]],\n",
      "\n",
      "         [[-3.5441e-01,  7.8861e-01,  1.3443e-01, -2.2383e-01]],\n",
      "\n",
      "         [[ 2.6491e-01,  1.2732e+00, -1.3109e-03, -3.0360e-01]]]],\n",
      "       grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614, 1.1172, 1.5804, 1.5584, 0.3253]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "block idx: q: 2 | kv: 2\n",
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0766, -0.1375, -0.8620, -0.5507]],\n",
      "\n",
      "         [[-0.1484,  0.8362,  0.2361, -0.3500]],\n",
      "\n",
      "         [[-0.3544,  0.7886,  0.1344, -0.2238]],\n",
      "\n",
      "         [[-0.1641,  0.8789,  0.1191,  0.0121]]]], grad_fn=<CopySlices>) | all global lse: tensor([[[  -inf, 0.6894, 1.0614, 1.1172, 1.5804, 1.5584, 0.7451]]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for bqi in range(nbq):\n",
    "    for bkvi in range(nbk):\n",
    "        print(f\"block idx: q: {bqi} | kv: {bkvi}\")\n",
    "        q_ = q[:, bqi*bq:(bqi+1)*bq, :, :]\n",
    "        k_ = k[:, bkvi*bkv:(bkvi+1)*bkv, :, :]\n",
    "        v_ = v[:, bkvi*bkv:(bkvi+1)*bkv, :, :]\n",
    "        # print(f\"q_.shape: {q_.shape} | k_.shape: {k_.shape} | v_.shape: {v_.shape}\")\n",
    "        on_swa(\n",
    "            q=q_, \n",
    "            k=k_,\n",
    "            v=v_,\n",
    "            global_o=o_,\n",
    "            global_lse=lse_,\n",
    "            block_idx_q=bqi,\n",
    "            block_idx_kv=bkvi\n",
    "        )\n",
    "        print(f\"all global o: {o_} | all global lse: {lse_}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all global o: tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0109, -0.3387, -1.3407, -0.5854]],\n",
      "\n",
      "         [[ 0.1202, -0.1787, -0.9227, -0.5619]],\n",
      "\n",
      "         [[ 0.0766, -0.1375, -0.8620, -0.5507]],\n",
      "\n",
      "         [[-0.1484,  0.8362,  0.2361, -0.3500]],\n",
      "\n",
      "         [[-0.3544,  0.7886,  0.1344, -0.2238]],\n",
      "\n",
      "         [[-0.1641,  0.8789,  0.1191,  0.0121]]]], grad_fn=<CopySlices>)\n",
      "\n",
      "all global lse: tensor([[[  -inf, 0.6894, 1.0614, 1.1172, 1.5804, 1.5584, 0.7451]]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"all global o: {o_}\\n\\nall global lse: {lse_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.testing import assert_close\n",
    "\n",
    "assert_close(o, o_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random global o, global lse, bqi, bkvi\n",
    "q, k, v, global_o, global_lse = construct_online_attn_args(\n",
    "    b, sq, skv, hq, hkv, hd, \n",
    "    bq, bkv, bqi_, bkvi_,\n",
    "    dtype=act_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_swa(\n",
    "    q, \n",
    "    k,\n",
    "    v,\n",
    "    global_o,\n",
    "    global_lse,\n",
    "    block_idx_q=bqi_,\n",
    "    block_idx_kv=bkvi_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all global o: tensor([[[[ 0.7262,  0.0912, -0.3891,  0.5279]],\n",
      "\n",
      "         [[ 1.0311, -0.7048,  1.0131, -0.3308]],\n",
      "\n",
      "         [[ 1.0950,  0.3399,  0.7200,  0.4114]],\n",
      "\n",
      "         [[-0.9727,  0.9585,  1.6192,  1.4506]],\n",
      "\n",
      "         [[ 0.2695, -0.2104, -0.7328,  0.1043]],\n",
      "\n",
      "         [[ 0.3488,  0.9676, -0.4657,  1.6048]],\n",
      "\n",
      "         [[-2.4801, -0.4175, -1.1955,  0.8123]]]], grad_fn=<CopySlices>)\n",
      "\n",
      "all global lse: tensor([[[0.9545, 0.6099, 0.5643, 0.0594, 0.7099, 0.4250, 0.2709]]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"all global o: {global_o}\\n\\nall global lse: {global_lse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 4, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,3,4]\n",
    "torch.concat([\n",
    "    torch.zeros(1, dtype=torch.int32, device=\"cpu\"),\n",
    "    torch.tensor(a, dtype=torch.int32, device=\"cpu\").cumsum(dim=0)\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, dtype=torch.float32)\n",
    "b = torch.randn(3, dtype=torch.bfloat16)\n",
    "\n",
    "(a * b).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([float('-inf'), float('-inf'), float('-inf')])\n",
    "F.softmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([float('-inf'), float('-inf'), float('-inf')])\n",
    "a - a + 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-inf, -inf, -inf])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([float('-inf'), float('-inf'), float('-inf')])\n",
    "def safe_subtract(\n",
    "    a: torch.Tensor,\n",
    "    b: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Safely subtracts two tensors.\n",
    "    where the subtraction results of two -inf will be set to -inf.\n",
    "    \"\"\"\n",
    "    eq = ((a == b) & (a == float('-inf'))).all(dim=-1, keepdim=True)\n",
    "    \n",
    "    sub = a - b\n",
    "    sub = torch.where(eq, torch.fill(sub, float('-inf')), sub)\n",
    "    \n",
    "    return sub\n",
    "\n",
    "safe_subtract(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [2.0561e-09, 9.9753e-01, 2.4726e-03],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def safe_softmax(a, dim=-1):\n",
    "    all_neg_inf = (a == float('-inf')).all(dim=dim, keepdim=True)\n",
    "    \n",
    "    sm = F.softmax(a, dim=dim)\n",
    "    \n",
    "    sm = torch.where(all_neg_inf, torch.zeros_like(sm), sm)\n",
    "\n",
    "    return sm\n",
    "\n",
    "# 示例\n",
    "a = torch.tensor([\n",
    "    [10, float('-inf'), float('-inf')],\n",
    "    [11, 31, 25],\n",
    "    [float('-inf'), float('-inf'), float('-inf')]\n",
    "])\n",
    "\n",
    "safe_softmax(a, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda q, kv, _v: (\n",
    "    q, \n",
    "    *torch.split(\n",
    "        kv, \n",
    "        split_size_or_sections=2,\n",
    "        dim=-2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(2, 4, 4)\n",
    "kv = torch.randn(2, 4, 4)\n",
    "\n",
    "f(q, kv, None)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x+1 for x in [1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4,5], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0117, 0.0317, 0.0861, 0.2341, 0.6364])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_a = F.softmax(a, dim=-1)\n",
    "sm_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4519)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse_a = a.exp().sum().log()\n",
    "lse_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4519)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse_a = torch.logsumexp(a, dim=-1)\n",
    "lse_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), tensor([4., 5.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = a[:3]\n",
    "a2 = a[3:]\n",
    "a1, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0900, 0.2447, 0.6652]), tensor([0.2689, 0.7311]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_a1 = F.softmax(a1, dim=-1)\n",
    "sm_a2 = F.softmax(a2, dim=-1)\n",
    "sm_a1, sm_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.4076), tensor(5.3133))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse_a1 = a1.exp().sum().log()\n",
    "lse_a2 = a2.exp().sum().log()\n",
    "lse_a1, lse_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.4076), tensor(5.3133))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse_a1 = torch.logsumexp(a1, dim=-1)\n",
    "lse_a2 = torch.logsumexp(a2, dim=-1)\n",
    "lse_a1, lse_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4519)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse_a_ = (lse_a1.exp() + lse_a2.exp()).log()\n",
    "lse_a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4519)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lse = torch.max(lse_a1, lse_a2)\n",
    "min_lse = torch.min(lse_a1, lse_a2)\n",
    "\n",
    "lse_a_ = max_lse + torch.log(1 + torch.exp(min_lse - max_lse)) # stable version\n",
    "lse_a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0117, 0.0317, 0.0861]), tensor([0.2341, 0.6364]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_a1_ = sm_a1 * (lse_a1 - lse_a_).exp()\n",
    "sm_a2_ = sm_a2 * (lse_a2 - lse_a_).exp()\n",
    "sm_a1_, sm_a2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([\n",
    "    [True, True, False]\n",
    "]).bool()\n",
    "\n",
    "b = torch.zeros((1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.masked_fill_(a, 1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf],\n",
       "        [0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [-inf, 0., 0., 0., -inf],\n",
       "        [-inf, -inf, 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sq, skv = 7, 5\n",
    "w = 2\n",
    "causal = True\n",
    "\n",
    "# init attn mask, with shape: [sq, skv]\n",
    "attn_mask = torch.zeros((sq, skv), dtype=torch.float)\n",
    "\n",
    "# init q row-index and k col-index\n",
    "maxs = max(sq, skv)\n",
    "qi = torch.arange(maxs-sq, maxs).view(-1, 1)  # [sq, 1]\n",
    "kj = torch.arange(maxs-skv, maxs).view(1, -1)  # [1, skv]\n",
    "\n",
    "w = w if w is not None else maxs\n",
    "\n",
    "# print(f\"qi: {qi}\")\n",
    "# print(f\"kj: {kj}\")\n",
    "\n",
    "# compute [lb, ub) of kj for each qi\n",
    "# non causal: [i-w, i] | causal: [i-w, i+w]\n",
    "lb = torch.clamp(\n",
    "    qi - w,\n",
    "    min=maxs-skv\n",
    ")\n",
    "ub = torch.clamp(\n",
    "    qi + w + 1,\n",
    "    max=maxs\n",
    ") if not causal else (qi + 1)\n",
    "\n",
    "# print(f\"lb: {lb}\")\n",
    "# print(f\"ub: {ub}\")\n",
    "\n",
    "# fill the attn mask\n",
    "# where '0' means the position to keep,\n",
    "# while '-inf' means the position to be masked out\n",
    "attn_mask.masked_fill_(\n",
    "    (kj < lb) | (kj >= ub),\n",
    "    float(\"-inf\")\n",
    ")\n",
    "\n",
    "# return with shape: (1, 1, sq, skv) to broadcast\n",
    "# attn_mask.unsqueeze(0).unsqueeze(0)\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
